---
title: 'Lab 4: Nonlinear Least Squares'
author: "Sophia Leiker"
date: "1/28/2022"
output: html_document
---

```{r setup, include=TRUE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(purrr)
library(tidyverse)
library(Metrics)
library(cowplot)
library(here)
```

## Introduction

To demonstrate the power of non linear least squares in R we're going to recreate a fisheries paper that examined whether productivity in fisheries was driven more by abundance, regime shifts, or simply random noise. [Here is a link to the paper if curious about detailied methods and results](https://www.pnas.org/content/110/5/1779). In their research, they used maximum likelihood estimation rather than non-linear least squares, but we will see similar results. In fact, the model choices for nls and mle are nearly identical in selected coefficients! Also to simplify the lab, we will only recreate the abundance and random models.

## Data Wrangling

### Ram Legacy Database

All data comes from the RAM Legacy Database, normally we would go through the whole database, but instead I have extracted out the main table containing all the values of stock parameters and a list of stocks within the cod and sole families to examine.

```{r}
timeseries_values_views<-read.csv(here("data", "timeseries_values_views"))

load("/Users/sophialeiker/Desktop/Bren/2_Winter_2022/244 - Advanced Data Analysis/labs/esm244-lab4-nonlinearleastsquares/data/stock_ids.Rdata")

```

This table is massive, so lets clean it up. For our analysis we are only interested in the stock name, year, biomass, and catch. Select those columns and remove any observations with `N`. Let's filter out stocks with less than 20 years of data to ensure we have enough observations for the nls models to converge. I'm going to add one more step and manually remove a few stocks that I know are undesirable. Mainly they collapse, have incorrect units or are redundant for this lab.

```{r}

## Remove stocks with less than 20 years of data
stock_id_clean<-timeseries_values_views %>% 
  filter(stockid %in% stock_ids$stockid) %>%
  select(stockid,year,TBbest,TCbest) %>% 
  drop_na() %>% 
  group_by(stockid) %>% 
  summarise(diff=max(year)-min(year)) %>% 
  filter(diff>20)

remove_vec=c(1,6,9,12,19,21,22,28,42,51,52,55)  # specific known stocks I want to remove

named_remove<-unique(stock_id_clean$stockid)[-remove_vec]  # Get a list of those names for filtering out

Fish_data<-timeseries_values_views %>% 
  filter(stockid %in% stock_id_clean$stockid) %>% 
  filter(stockid %in% named_remove)
  
```

## Single model NLS

Surplus is the excess amount of biomass that was added or taken from the underlying stock. It can be modeled as a simple addition. Surplus also allows us to generally model recruitment, growth, and natural mortality that is often difficult data to collect. Stock assessements, that RAM is built on, allows us to easily back out suprlus.

\begin{equation}
S_t=B_{t+1}-B_t+C_t
\end{equation}

We will need to add a column in our dataset calculating surplus in any given year. Since we have a variable from the future we can use the `lead()` function. Make sure to drop the `NA` created by the ahead function.

```{r}
surplus<-Fish_data %>% 
  group_by(stockid) %>% 
  select(stockid,year,TBbest,TCbest) %>% 
  drop_na() %>% 
  mutate(f_biomass=lead(TBbest)) %>% 
  mutate(surplus=f_biomass-TBbest+TCbest) %>% 
  drop_na()
  
```

Let's see what our data looks like with an example of one stock.

```{r}
one_stock<-surplus %>% 
  filter(stockid=="COD1f-XIV")

ggplot(data=one_stock,aes(x=year,y=surplus))+
  geom_point(size=3,color="black")+
  theme_minimal()

```

### Create a Fox Model

There are three primary surplus-production models in the fishery world. The most common is the Gordon-Schaefer model. Vert-pre etal., use a Fox-Model that typically provides a more conservative estimate of maximum sustainable yield. The last model is the Pella-Tomslison model that really is just a more flexible model of the other too using a shape parameter $\phi$ to control the curve. All are built on a logistic growth curve. Given a level of biomass we will be able to predict what the surplus ought to be if we know (or will determine) the maximum sustainable yield and the carrying capacity. Maximum sustainable yield simply refers to the amount of biomass that facilitates the greatest level of harvest possible without depleting the stock. Carrying capacity is the upper bound on the total population size and represents natural environmental pressure limiting stock growth. The paper uses a simplified Fox model that we try to find parameters for to fit the fishery data.  

\begin{equation}
\hat{S_t}=-e*MSY(\frac{B_t}{K})\ln(\frac{B_t}{K})
\end{equation}

Where e is base of the natural log $\approx$ 2.718, MSY is the maximum sustainable yield, K is the carrying capacity, and $B_t$ is the biomass for the observed year.

Let's create a function in R.

```{r foxmodel}
fox<-function(m,carry,biomass){
 out= -2.718*m*(biomass/carry)*log(biomass/carry)
return(out)
}
```


Now we can construct our nonlinear least squares with sufficient guesses. But what should our guesses be? Well carrying capacity is straightforward. Traditionally, its estimated as the highest observed biomass so we can just the max of the biomass data. Maximum sustainable yield can be found through analytical analysis. It's been done many times over so I'll just tell you it's estimated at 37% of the carrying capacity.

### Single species NLS

```{r nlsonemodel}
#Write out the guess first, we'll move into the nls wrapper soon

guess_vec=c(max(one_stock$TBbest)*0.37,max(one_stock$TBbest))

one_stock_nls=nls(surplus~fox(m,carry,TBbest),
                  data=one_stock,
                  start=list(m=guess_vec[1],carry=guess_vec[2]),trace=TRUE )

#can check the output by doing summary(one_stock_nls)
```

Great our model works on a single model! Now we need to find a way to replicate the analysis. Ideally without using for loops as those can be a pain to account for. 

### Using purrr to run many nls models

Purrr is a package in r that has been designed to use the functionality of lapply family of equations for the tidyverse. So now we can use pipes to pass along the application of functions and models to dataframes, specific lists in a dataframe, or specific indices of a dataframe. There is some new syntax that we will use, but hopefully it will be clear and you'll the power of purrr in future applications.

# Note: The map functions transform their input by applying a function to each element of a list or atomic vector and returning an object of the same length as the input.

```{r nlsmany}
#Define a new function to pass along the nls calls
#this directly tells what to run the nls function on it

all_nls_fcn<-function(surplus_df){
  nls(surplus~fox(m,carry,TBbest),
  data=surplus_df,
  start=list(m=max(surplus_df$TBbest)*0.37,carry=max(surplus_df$TBbest)))
}

## Pay attention to the position and use of .x, .y, and .f in the map functions


fox_all<-surplus %>%
  group_by(stockid) %>% 
  nest() %>% #rather than applying a summary, this packs all the results into a list (this condenses things down into lists rather than a super long dataframe)
  mutate(nls_model=map(data,~all_nls_fcn(.x))) %>% 
  mutate(predictions=map2(nls_model,data,~predict(.x,newdata=.y))) %>% 
  mutate(RMSE=map2_dbl(predictions,data,~rmse(.x,.y$surplus))) #adding the extra .y term allows us to pass in 2 inputs into the model
#this adds an extra column to the df with the incorporation of a vector of predictions, took the models we made, and made predictions for each of the models with the underlying stock tables
#map2_dbl --> we just want a single number so we want the function to spit out a nice easy number
#take function and make the predictions
#in y$surplus we are using that because the surplus is contained (stacked) within that column

#predict (predict function, the df, the evaluation metric)
#map function --> applying nls to every spot in the df (the condensed list form), this adds a new column to the df which contains the model object within each of the rows containing the model outputs

```

## Compare to a random null model

In the paper they derive a null model to test the different models against. The best way to test if any of these models are better is if they can out perform just a random collection of data. They propose if we just use the average surplus from the time period and our models can't outpeform that, then the stock is under more influence of sheer randomness then any explicable measures. We can jump straight into the purrr analysis.

```{r}

# Define the model, don't worry to much how I got it what it means
r_avg<-function(surplus){
  avg_sur=mean(surplus)
  
  rmse=sqrt(mean((avg_sur-surplus)^2))
  
  return(rmse)
}


r_mse<-surplus %>%
  group_by(stockid) %>% 
  nest() %>% #nice condensed data frame
  mutate(RMSE=map_dbl(data,~r_avg(.x$surplus)))
#Creates new column with RMSE
```

## How did the models compare to the null?

```{r}
which(r_mse$RMSE-fox_all$RMSE<0)
#for which of the stocks did the null model perform better than the fox model

#out models for fox are significantly better than the null model

fox_all$stockid[39]
#this is the specific stock just to pull it out and show an example


```
In the paper, about 12% of the stocks were more explained by random shocks and 16% more so by abundance models. The rest were lead by regime shifts that we did not model. Our results only found one stock out of 44 was better explained by random growth. Either our choice of nls models is far better than their mle method (not likely), or the subset I choose lends itself more to abundance models (this is really what happened). I did not want to overwhelm the analysis with over 200 stocks. 

## Graph the top 5 best fit models

Purrr combined with cowplot creates a streamlined way to build multiple graphs. Let's take the 5 best fit fox models and show how they performed compared to the data. 

```{r}
plots<-fox_all %>% 
  arrange(RMSE) %>% #can change to arrange_decending to flip to show the bad ones
  head(5) %>% 
  mutate(graph=map2(data,predictions,~ggplot()+geom_point(data = .x,aes(x=.x$year,y=.x$surplus,color='Actual'))+geom_point(aes(x=.x$year,y=.y,color='Predicted'))+theme_minimal()+xlab('Year')+ylab('Surplus')+scale_color_manual(name="Legend",breaks = c('Actual','Predicted'),values=c('Actual'='black','Predicted'='red'))))

#breaks separates vectors of actual and predicted 
#this breaks of the color of the dots to actual and predicted

#this adds another column in the dataframe to create a graph

legend<-get_legend(plots$graph[[1]]) #this is removing the legend and putting it in the lower right hand corner (this is extracting the legend)

for(i in 1:length(plots$graph)){
  plots$graph[[i]]<-plots$graph[[i]]+theme(legend.position = "none") #this is removing the legends
}

plot_list=plots$graph
#this is pulling out the images from the plots column df

plot_list[[6]]<-legend #this is defining the 6th spot to put the legend

cowplot::plot_grid(plotlist=plot_list,labels =c( plots$stockid,""),hjust=-0.5)
```




